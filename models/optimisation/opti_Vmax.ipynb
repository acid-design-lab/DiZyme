{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from numpyencoder import NumpyEncoder\n",
    "import lightgbm as lgb\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('multi_final.csv')\n",
    "with open('features_multy.txt', 'r') as file:\n",
    "    features = [feat.replace('\\n', '') for feat in file]\n",
    "\n",
    "Vmax_df = df[features]\n",
    "Vmax_df['Vmax'] = df['Vmax']\n",
    "Vmax_target = np.log10(Vmax_df['Vmax'])\n",
    "Vmax_features = Vmax_df.drop(['Vmax'], axis = 1)\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_filled = imputer.fit_transform(Vmax_features)\n",
    "Vmax_features = pd.DataFrame(df_filled, columns=Vmax_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def HistGradientBoostingRegressor_vae(features_, target_,\n",
    "                                scaler = MinMaxScaler(),\n",
    "                                use_scaler=True,\n",
    "                                random_st = 20,\n",
    "                                n_cv = 5,\n",
    "                                shuffle_on = True,\n",
    "                                objective_st='binary',\n",
    "                                learning_rat=0.1,\n",
    "                                n_boost_round=500\n",
    "                                ):\n",
    "\n",
    "  param_gr = {\n",
    "     'learning_rate': np.array([0.01, 0.08, 0.05, 0.1, 0.23 , 0.34]),\n",
    "      'min_samples_leaf': np.array([5, 10, 20, 30]),\n",
    "      'max_depth': np.array([4, 8, 12, -1], dtype = int),\n",
    "      'max_bins': np.array([100, 200, 255]),\n",
    "      'min_samples_leaf': np.array([10, 20, 25]),\n",
    "      }\n",
    "\n",
    "  scaler.fit(features_)\n",
    "  sc_feature = scaler.transform(features_)\n",
    "  gkf = KFold(n_splits=n_cv, shuffle=shuffle_on, random_state=random_st).split(features_, target_)\n",
    "\n",
    "\n",
    "  lgb_estimator = HistGradientBoostingRegressor()\n",
    "\n",
    "  gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_gr, cv=gkf, verbose = 2)\n",
    "  lgb_model = gsearch.fit(sc_feature, target_)\n",
    "\n",
    "  return lgb_model.best_params_, lgb_model.best_score_\n",
    "\n",
    "\n",
    "def HistGradientBoostingRegressor_Learning(features, target, target_name, best_grid,\n",
    "                            scaler = MinMaxScaler(),\n",
    "                            use_scaler=True,\n",
    "                            random_st = 20,\n",
    "                            size_of_test=0.2,\n",
    "                            filename_for_wght = '.pkl',\n",
    "                            filename_for_scaler = '.pkl'\n",
    "                            ):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = size_of_test, random_state = random_st)\n",
    "\n",
    "        scaler.fit(X_train)\n",
    "        x_train = scaler.transform(X_train)\n",
    "        x_test = scaler.transform(X_test)\n",
    "        joblib.dump(scaler, filename_for_scaler)\n",
    "\n",
    "        lgb_model = HistGradientBoostingRegressor(**best_grid)\n",
    "        lgb_model.fit(x_train, y_train)\n",
    "        joblib.dump(lgb_model, filename_for_wght)\n",
    "\n",
    "        lgb_model = joblib.load(filename_for_wght)\n",
    "        y_pred = lgb_model.predict(x_test)\n",
    "        print(\"R2__score\")\n",
    "        print(r2_score(y_test, y_pred))\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def LGBMRegressor_grid_cv(features, target,\n",
    "                                scaler = MinMaxScaler(),\n",
    "                                use_scaler=True,\n",
    "                                random_st = 20,\n",
    "                                n_cv = 5,\n",
    "                                shuffle_on = True,\n",
    "                                n_boost_round=500\n",
    "                                ):\n",
    "\n",
    "        param_gr = {\n",
    "            'num_leaves': [4, 8, 16,20, 25,30],\n",
    "            'n_estimators': np.array([50, 100, 200 , 500, 700], dtype = int),\n",
    "            'learning_rate': np.array([0.01, 0.08, 0.05, 0.1, 0.23 , 0.34]),\n",
    "            'max_depth': np.array([4, 8, 12, -1], dtype = int),\n",
    "            'class_weight': [\"balanced\", None],\n",
    "            'reg_alpha': np.array([0, 0.1, 1]),\n",
    "            'reg_lambda': np.array([0, 0.1, 1])\n",
    "            }\n",
    "\n",
    "\n",
    "        scaler.fit(features)\n",
    "        sc_feature = scaler.transform(features)\n",
    "        gkf = KFold(n_splits=n_cv, shuffle=shuffle_on, random_state=random_st).split(features, target)\n",
    "\n",
    "\n",
    "        lgb_estimator = lgb.LGBMRegressor(boosting_type='gbdt', num_boost_round=n_boost_round)\n",
    "\n",
    "        gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_gr, cv=gkf)\n",
    "        lgb_model = gsearch.fit(sc_feature, target)\n",
    "\n",
    "        return lgb_model.best_params_, lgb_model.best_score_\n",
    "\n",
    "def LGBMRegressor_Learning(features, target, target_name, best_grid,\n",
    "                            scaler = MinMaxScaler(),\n",
    "                            use_scaler=True,\n",
    "                            random_st = 20,\n",
    "                            size_of_test=0.2,\n",
    "                            filename_for_wght = '.pkl',\n",
    "                            filename_for_scaler = '.pkl'\n",
    "                            ):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = size_of_test, random_state = random_st)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.fit(X_train)\n",
    "            x_train = scaler.transform(X_train)\n",
    "            x_test = scaler.transform(X_test)\n",
    "            joblib.dump(scaler, filename_for_scaler)\n",
    "\n",
    "            lgb_model = lgb.LGBMRegressor(**best_grid)\n",
    "            lgb_model.fit(x_train, y_train)\n",
    "            joblib.dump(lgb_model, filename_for_wght)\n",
    "\n",
    "\n",
    "            lgb_model = joblib.load(filename_for_wght)\n",
    "            y_pred = lgb_model.predict(x_test)\n",
    "            print(\"R2__score\")\n",
    "            print(r2_score(y_test, y_pred))\n",
    "            return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def RandomForestRegressor_grid_cv(features, target,\n",
    "                                scaler = MinMaxScaler(),\n",
    "                                use_scaler=True,\n",
    "                                random_st = 20,\n",
    "                                n_cv = 5,\n",
    "                                shuffle_on = True,\n",
    "                                learning_rat=0.1,\n",
    "                                ):\n",
    "\n",
    "        param_gr = {\n",
    "        \"n_estimators\": np.array(range(100,701, 100)),\n",
    "        \"min_samples_split\": np.array([2, 5, 10, 20, 30, 50]),\n",
    "        \"min_samples_leaf\": np.array([2, 5, 10, 20, 30, 50]),\n",
    "        \"max_depth\": np.array([5, 10, 16, 32, 64, 80, 100]),\n",
    "        \"max_features\": np.array([\"sqrt\", \"log2\", None]),\n",
    "    }\n",
    "\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.fit(features)\n",
    "            sc_feature = scaler.transform(features)\n",
    "            gkf = KFold(n_splits=n_cv, shuffle=shuffle_on, random_state=random_st).split(features, target)\n",
    "\n",
    "\n",
    "            rf_estimator = RandomForestRegressor()\n",
    "\n",
    "            gsearch = GridSearchCV(estimator=rf_estimator, param_grid=param_gr, cv=gkf, verbose = 2)\n",
    "            rf_model = gsearch.fit(sc_feature, target)\n",
    "\n",
    "            return rf_model.best_params_, rf_model.best_score_\n",
    "\n",
    "def RandomForestRegressor_Learning(features, target, target_name, best_grid,\n",
    "                            scaler = MinMaxScaler(),\n",
    "                            use_scaler=True,\n",
    "                            random_st = 20,\n",
    "                            size_of_test=0.2,\n",
    "                            filename_for_wght = '.pkl',\n",
    "                            filename_for_scaler = '.pkl'\n",
    "                            ):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = size_of_test, random_state = random_st)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.fit(X_train)\n",
    "            x_train = scaler.transform(X_train)\n",
    "            x_test = scaler.transform(X_test)\n",
    "            joblib.dump(scaler, filename_for_scaler)\n",
    "\n",
    "            model = RandomForestRegressor(**best_grid)\n",
    "            model.fit(x_train, y_train)\n",
    "            joblib.dump(model, filename_for_wght)\n",
    "\n",
    "\n",
    "            model = joblib.load(filename_for_wght)\n",
    "            y_pred = model.predict(x_test)\n",
    "            print(\"R2__score\")\n",
    "            print(r2_score(y_test, y_pred))\n",
    "            return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ExtraTreesRegressor_grid_cv(features, target,\n",
    "                                scaler = MinMaxScaler(),\n",
    "                                use_scaler=True,\n",
    "                                random_st = 20,\n",
    "                                n_cv = 5,\n",
    "                                shuffle_on = True,\n",
    "                                learning_rat=0.1,\n",
    "                                ):\n",
    "\n",
    "        param_gr = {\n",
    "        \"n_estimators\": np.array(range(100,701, 100)),\n",
    "        \"min_samples_split\": np.array([2, 5, 10, 20, 30, 50]),\n",
    "        \"min_samples_leaf\": np.array([2, 5, 10, 20, 30, 50]),\n",
    "        \"max_depth\": np.array([5, 10, 16, 32, 64, 80, 100]),\n",
    "        \"max_features\": np.array([\"sqrt\", \"log2\", None]),\n",
    "        \"ccp_alpha\" : np.array([0, 0.01, 0.1])\n",
    "    }\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.fit(features)\n",
    "            sc_feature = scaler.transform(features)\n",
    "            gkf = KFold(n_splits=n_cv, shuffle=shuffle_on, random_state=random_st).split(features, target)\n",
    "\n",
    "\n",
    "            rf_estimator = ExtraTreesRegressor()\n",
    "\n",
    "            gsearch = GridSearchCV(estimator=rf_estimator, param_grid=param_gr, cv=gkf, verbose = 2)\n",
    "            rf_model = gsearch.fit(sc_feature, target)\n",
    "\n",
    "            return rf_model.best_params_, rf_model.best_score_\n",
    "\n",
    "def ExtraTreesRegressor_Learning(features, target, target_name, best_grid,\n",
    "                            scaler = MinMaxScaler(),\n",
    "                            use_scaler=True,\n",
    "                            random_st = 20,\n",
    "                            size_of_test=0.2,\n",
    "                            filename_for_wght = '.pkl',\n",
    "                            filename_for_scaler = '.pkl'\n",
    "                            ):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = size_of_test, random_state = random_st)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.fit(X_train)\n",
    "            x_train = scaler.transform(X_train)\n",
    "            x_test = scaler.transform(X_test)\n",
    "            joblib.dump(scaler, filename_for_scaler)\n",
    "\n",
    "            model = ExtraTreesRegressor(**best_grid)\n",
    "            model.fit(x_train, y_train)\n",
    "            joblib.dump(model, filename_for_wght)\n",
    "\n",
    "\n",
    "            model = joblib.load(filename_for_wght)\n",
    "            y_pred = model.predict(x_test)\n",
    "            print(\"R2__score\")\n",
    "            print(r2_score(y_test, y_pred))\n",
    "            return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CatBoostRegressor_grid_cv(features, target,\n",
    "                                scaler = MinMaxScaler(),\n",
    "                                random_st = 20,\n",
    "                                n_cv = 5,\n",
    "                                shuffle_on = True,\n",
    "                                learning_rat=0.1,\n",
    "                                ):\n",
    "\n",
    "        param_gr = {\n",
    "        'depth'         : [3, 6, 8, 10, 12, 15],\n",
    "        'learning_rate' : [0.01, 0.05, 0.1, 0.5, 1],\n",
    "        'iterations'    : [30, 50, 100, 200, 500],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9, 13, 15]\n",
    "    }\n",
    "\n",
    "        scaler.fit(features)\n",
    "        sc_feature = scaler.transform(features)\n",
    "\n",
    "        gkf = KFold(n_splits=n_cv, shuffle=shuffle_on, random_state=random_st).split(features, target)\n",
    "\n",
    "        catboost_estimator = CatBoostRegressor()\n",
    "\n",
    "        gsearch = GridSearchCV(estimator=catboost_estimator, param_grid=param_gr, cv=gkf, verbose = 2)\n",
    "        catboost_model = gsearch.fit(sc_feature, target)\n",
    "\n",
    "        return catboost_model.best_params_, catboost_model.best_score_\n",
    "\n",
    "def CatBoostRegressor_Learning(features, target, target_name, best_grid,\n",
    "                            scaler = MinMaxScaler(),\n",
    "                            use_scaler=True,\n",
    "                            random_st = 20,\n",
    "                            size_of_test=0.2,\n",
    "                            filename_for_wght = '.pkl',\n",
    "                            filename_for_scaler = '.pkl'\n",
    "                            ):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = size_of_test, random_state = random_st)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.fit(X_train)\n",
    "            x_train = scaler.transform(X_train)\n",
    "            x_test = scaler.transform(X_test)\n",
    "            joblib.dump(scaler, filename_for_scaler)\n",
    "\n",
    "            model = CatBoostRegressor(**best_grid)\n",
    "            model.fit(x_train, y_train)\n",
    "            joblib.dump(model, filename_for_wght)\n",
    "\n",
    "\n",
    "            model = joblib.load(filename_for_wght)\n",
    "            y_pred = model.predict(x_test)\n",
    "            print(\"R2__score\")\n",
    "            print(r2_score(y_test, y_pred))\n",
    "            return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def AdaBoostRegressor_grid_cv(features, target,\n",
    "                                scaler = MinMaxScaler(),\n",
    "                                random_st = 20,\n",
    "                                n_cv = 5,\n",
    "                                shuffle_on = True,\n",
    "                                learning_rat=0.1,\n",
    "                                ):\n",
    "\n",
    "        param_gr = {\n",
    "                   'n_estimators': [10, 30, 50,70, 100, 300, 500],\n",
    "                   'learning_rate': [0.001, 0.005,0.01, 0.05, 0.1, 0.5, 1],\n",
    "                   'loss': ['linear', 'square', 'exponential']\n",
    "                    }\n",
    "\n",
    "        scaler.fit(features)\n",
    "        sc_feature = scaler.transform(features)\n",
    "\n",
    "        gkf = KFold(n_splits=n_cv, shuffle=shuffle_on, random_state=random_st).split(features, target)\n",
    "\n",
    "        adaboost_estimator = AdaBoostRegressor()\n",
    "\n",
    "        gsearch = GridSearchCV(estimator=adaboost_estimator, param_grid=param_gr, cv=gkf, verbose = 2)\n",
    "        adaboost_model = gsearch.fit(sc_feature, target)\n",
    "\n",
    "        return adaboost_model.best_params_, adaboost_model.best_score_\n",
    "\n",
    "def AdaBoostRegressor_Learning(features, target, target_name, best_grid,\n",
    "                            scaler = MinMaxScaler(),\n",
    "                            use_scaler=True,\n",
    "                            random_st = 20,\n",
    "                            size_of_test=0.2,\n",
    "                            filename_for_wght = '.pkl',\n",
    "                            filename_for_scaler = '.pkl'\n",
    "                            ):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = size_of_test, random_state = random_st)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.fit(X_train)\n",
    "            x_train = scaler.transform(X_train)\n",
    "            x_test = scaler.transform(X_test)\n",
    "            joblib.dump(scaler, filename_for_scaler)\n",
    "\n",
    "            model = AdaBoostRegressor(**best_grid)\n",
    "            model.fit(x_train, y_train)\n",
    "            joblib.dump(model, filename_for_wght)\n",
    "\n",
    "\n",
    "            model = joblib.load(filename_for_wght)\n",
    "            y_pred = model.predict(x_test)\n",
    "            print(\"R2__score\")\n",
    "            print(r2_score(y_test, y_pred))\n",
    "            return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_hgb, score_hgb = HistGradientBoostingRegressor_vae(features_ = Vmax_features, target_ = Vmax_target)\n",
    "with open(\"Vmax_params_hgb_multi.json\", 'w', encoding=\"utf8\") as fp:\n",
    "    json.dump(params_hgb, fp, ensure_ascii=False, indent=4, cls = NumpyEncoder)\n",
    "\n",
    "HistGradientBoostingRegressor_Learning(Vmax_features, Vmax_target, 'Vmax', params_hgb,random_st = 20,\n",
    "                            filename_for_wght = 'Vmax_model_multi_hgb.pkl',\n",
    "                            filename_for_scaler = 'Vmax_scaler_multi_hgb.pkl'\n",
    "                                       )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_lgb, score_lgb = LGBMRegressor_grid_cv(Vmax_features, Vmax_target)\n",
    "with open(\"Vmax_params_lgb_multi.json\", 'w', encoding=\"utf8\") as fp:\n",
    "    json.dump(params_lgb, fp, ensure_ascii=False, indent=4, cls = NumpyEncoder)\n",
    "\n",
    "LGBMRegressor_Learning(Vmax_features, Vmax_target, 'Vmax', params_lgb, random_st = 20,\n",
    "                            filename_for_wght = 'Vmax_model_multi_lgb.pkl',\n",
    "                            filename_for_scaler = 'Vmax_scaler_multi_lgb.pkl'\n",
    "                                       )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_rfr, score_rfr = RandomForestRegressor_grid_cv(Vmax_features, Vmax_target)\n",
    "with open(\"Vmax_params_rfr_multi.json\", 'w', encoding=\"utf8\") as fp:\n",
    "    json.dump(params_rfr, fp, ensure_ascii=False, indent=4, cls = NumpyEncoder)\n",
    "\n",
    "RandomForestRegressor_Learning(Vmax_features, Vmax_target, 'Vmax', params_rfr, random_st = 20,\n",
    "                            filename_for_wght = 'Vmax_model_multi_rfr.pkl',\n",
    "                            filename_for_scaler = 'Vmax_scaler_multi_rfr.pkl'\n",
    "                                       )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_etr, score_etr = ExtraTreesRegressor_grid_cv(Vmax_features, Vmax_target)\n",
    "with open(\"Vmax_params_etr_multi.json\", 'w', encoding=\"utf8\") as fp:\n",
    "    json.dump(params_etr, fp, ensure_ascii=False, indent=4, cls = NumpyEncoder)\n",
    "\n",
    "ExtraTreesRegressor_Learning(Vmax_features, Vmax_target, 'Vmax', params_etr, random_st = 20,\n",
    "                            filename_for_wght = 'Vmax_model_multi_etr.pkl',\n",
    "                            filename_for_scaler = 'Vmax_scaler_multi_etr.pkl'\n",
    "                                       )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_cbr, score_cbr = CatBoostRegressor_grid_cv(Vmax_features, Vmax_target)\n",
    "with open(\"Vmax_params_cbr_multi.json\", 'w', encoding=\"utf8\") as fp:\n",
    "    json.dump(params_cbr, fp, ensure_ascii=False, indent=4, cls = NumpyEncoder)\n",
    "\n",
    "CatBoostRegressor_Learning(Vmax_features, Vmax_target, 'Vmax', params_cbr, random_st = 20,\n",
    "                            filename_for_wght = 'Vmax_model_multi_cbr.pkl',\n",
    "                            filename_for_scaler = 'Vmax_scaler_multi_cbr.pkl'\n",
    "                                       )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_abr, score_abr = AdaBoostRegressor_grid_cv(Vmax_features, Vmax_target)\n",
    "with open(\"Vmax_params_abr_multi.json\", 'w', encoding=\"utf8\") as fp:\n",
    "    json.dump(params_abr, fp, ensure_ascii=False, indent=4, cls = NumpyEncoder)\n",
    "\n",
    "AdaBoostRegressor_Learning(Vmax_features, Vmax_target, 'Vmax', params_abr, random_st = 20,\n",
    "                            filename_for_wght = 'Vmax_model_multi_abr.pkl',\n",
    "                            filename_for_scaler = 'Vmax_scaler_multi_abr.pkl'\n",
    "                                       )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
